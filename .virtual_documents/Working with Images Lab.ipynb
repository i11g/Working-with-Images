import numpy as np
import os, glob, math, json, csv
import matplotlib.pyplot as plt

import pandas as pd
from PIL import Image
import imagehash
from collections import defaultdict, Counter
import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet50 import ResNet50, decode_predictions
from tensorflow.keras.applications.resnet50 import preprocess_input
from sklearn.metrics import confusion_matrix, classification_report

















os.listdir("data/blackfoot-cat")


os.listdir("data/chinese-mountain-cat")


os.listdir("data/domestic-cat")


os.listdir("data/jungle-cat")


os.listdir("data/european-wildcat")


os.listdir("data/sand-cat")


os.listdir("data/african-wildcat")





files1 = os.listdir("data/african-wildcat")
files2 = os.listdir("data/blackfoot-cat")
files3 = os.listdir("data/chinese-mountain-cat")
files4 = os.listdir("data/domestic-cat")
files5 = os.listdir("data/european-wildcat")
files6 = os.listdir("data/jungle-cat")
files7 = os.listdir("data/sand-cat")
cat_files = files1 + files2 + files3 + files4 + files5 + files6 + files7


# Supported image extensions
IMAGE_EXTENSIONS = ('.jpg', '.jpeg', '.png')

# Folders to scan
folders = [
    "data/african-wildcat",
    "data/blackfoot-cat",
    "data/sand-cat",
    "data/chinese-mountain-cat",
    "data/domestic-cat",
    "data/european-wildcat",
    "data/jungle-cat"
]

# Step 1: Collect image paths and sizes
image_data = []
for folder in folders:
    for file in os.listdir(folder):
        full_path = os.path.join(folder, file)
        if os.path.isfile(full_path) and file.lower().endswith(IMAGE_EXTENSIONS):
            try:
                with Image.open(full_path) as img:
                    width, height = img.size
                    image_data.append((full_path, width, height))
            except Exception as e:
                print(f"Error reading {full_path}: {e}")

# Step 2: Create DataFrame
df = pd.DataFrame(image_data, columns=["path", "width", "height"])

# Step 3: Count most common sizes
size_counts = Counter([(w, h) for _, w, h in image_data])
most_common = size_counts.most_common()

print("Most common image sizes (width x height):")
for size, count in most_common[:10]:  # show top 10
    print(f"{size[0]} x {size[1]} - {count} images")

# Step 4: Average size
if not df.empty:
    avg_width = df["width"].mean()
    avg_height = df["height"].mean()
    print(f"\n Average size: {avg_width:.1f} x {avg_height:.1f} pixels")


# Step 1: Compute area and size
df["area"] = df["width"] * df["height"]
df["size"] = list(zip(df["width"], df["height"]))

# Step 2: Group by size and count
size_counts = df.groupby("size").agg(
    count=("size", "count"),
    width=("width", "first"),
    height=("height", "first"),
    area=("area", "first")
).reset_index(drop=True)

# Step 3: IQR outlier detection function
def detect_outliers(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    return (series < Q1 - 1.5 * IQR) | (series > Q3 + 1.5 * IQR)

# Step 4: Apply IQR on width & height
size_counts["outlier_width"] = detect_outliers(size_counts["width"])
size_counts["outlier_height"] = detect_outliers(size_counts["height"])
size_counts["is_outlier"] = size_counts["outlier_width"] | size_counts["outlier_height"]

# Step 5: Filter to only outliers with high counts
threshold = 3
filtered_outliers = size_counts[(size_counts["is_outlier"]) & (size_counts["count"] >= threshold)]

# Step 6: Show outlier size summaries
print(f"\n Outlier sizes that appear at least {threshold} times:")
for _, row in filtered_outliers.iterrows():
    print(f"{row['width']} x {row['height']} - {row['count']} images")

# Step 7: Get actual image paths from df
outlier_sizes = set(zip(filtered_outliers["width"], filtered_outliers["height"]))
outlier_df = df[df["size"].isin(outlier_sizes)]

print(f"\n Total outlier images after filtering: {len(outlier_df)}")
print(outlier_df[["path", "width", "height"]].to_string(index=False))





folders = [
    "data/african-wildcat",
    "data/blackfoot-cat",
    "data/chinese-mountain-cat",
    "data/domestic-cat",
    "data/european-wildcat",
    "data/jungle-cat",
    "data/sand-cat"
]


get_ipython().getoutput("pip install imagehash pillow")


IMAGE_EXTENSIONS = ('.jpg', '.jpeg', '.png')
folders = [
    "data/african-wildcat",
    "data/blackfoot-cat",
    "data/chinese-mountain-cat",
    "data/domestic-cat",
    "data/european-wildcat",
    "data/jungle-cat",
    "data/sand-cat"
]

hash_dict = defaultdict(list)
duplicate_paths = []

for folder in folders:
    for fname in os.listdir(folder):
        path = os.path.join(folder, fname)
        if os.path.isfile(path) and fname.lower().endswith(IMAGE_EXTENSIONS):
            try:
                with Image.open(path) as img:
                    hash_val = imagehash.phash(img)  # perceptual hash
                    if hash_val in hash_dict:
                        duplicate_paths.append(path)  # This is a duplicate
                    else:
                        hash_dict[hash_val].append(path)
            except Exception as e:
                print(f"❌ Error with file {path}: {e}")

print(f"✅ Found {len(duplicate_paths)} duplicates:")
for dup in duplicate_paths:
    print(dup)





model = ResNet50(weights='imagenet', include_top=True)


model.summary()


print(f"\n🔍 Total layers: {len(model.layers)}")
print(f"🧮 Total parameters: {model.count_params():,}")





# === TensorFlow: Efficient batched preprocessing + optional shard saving ===
import os, glob, json
import numpy as np
import tensorflow as tf
from tensorflow.keras.applications.resnet50 import preprocess_input

# ------------------- CONFIG -------------------
DATA_DIR   = "data"          # folder with class subfolders, e.g. data/class1, data/class2, ...
IMG_SIZE   = (224, 224)      # ResNet50 expects 224x224
BATCH_SIZE = 4               # task requirement
SHARD_SIZE = 64              # how many samples per .npz file when saving
OUT_DIR    = "preprocessed_npz_tf"  # where to write shards
SAVE_SHARDS = True           # set False if you only want the streaming dataset
CACHE_IN_RAM = False         # set True to cache preprocessed tensors in RAM (small datasets only)
# ----------------------------------------------

# 1) Build (file_path, label) lists
classes = sorted([d for d in tf.io.gfile.listdir(DATA_DIR)
                  if tf.io.gfile.isdir(tf.io.gfile.join(DATA_DIR, d))])
class_to_idx = {c: i for i, c in enumerate(classes)}

file_paths, labels = [], []
for c in classes:
    for p in sorted(glob.glob(os.path.join(DATA_DIR, c, "*"))):
        file_paths.append(p)
        labels.append(class_to_idx[c])

print(f"Found {len(file_paths)} images across {len(classes)} classes: {classes}")

# 2) tf.data pipeline: stream -> decode -> resize -> preprocess -> batch -> prefetch
paths_ds  = tf.data.Dataset.from_tensor_slices(file_paths)
labels_ds = tf.data.Dataset.from_tensor_slices(labels)
ds = tf.data.Dataset.zip((paths_ds, labels_ds))

def load_and_preprocess(path, label):
    img_bytes = tf.io.read_file(path)
    img = tf.image.decode_image(img_bytes, channels=3, expand_animations=False)   # supports jpg/png
    img = tf.image.resize(img, IMG_SIZE)                                         # (224,224,3)
    img = tf.cast(img, tf.float32)
    img = preprocess_input(img)                                                  # ResNet50 normalization
    return img, label

ds = ds.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)
if CACHE_IN_RAM:
    ds = ds.cache()  # keeps preprocessed tensors in RAM; fine for small datasets
ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# Quick sanity check: load one batch
for xb, yb in ds.take(1):
    print("One batch:", xb.shape, yb.shape)  # (4, 224, 224, 3) and (4,)
    break

# 3) (Optional) Save preprocessed arrays to .npz shards for lazy loading later
if SAVE_SHARDS:
    tf.io.gfile.makedirs(OUT_DIR)
    with tf.io.gfile.GFile(os.path.join(OUT_DIR, "labels.json"), "w") as f:
        json.dump({"classes": classes, "class_to_idx": class_to_idx}, f, indent=2)

    shard_imgs, shard_labs = [], []
    shard_idx, written = 0, 0
    for xb, yb in ds:  # xb (N,224,224,3) float32 already normalized; yb (N,)
        xb_np = xb.numpy()
        yb_np = yb.numpy().astype(np.int64)
        for i in range(len(xb_np)):
            shard_imgs.append(xb_np[i])
            shard_labs.append(yb_np[i])
            written += 1
            if len(shard_imgs) == SHARD_SIZE:
                np.savez_compressed(
                    os.path.join(OUT_DIR, f"shard_{shard_idx:05d}.npz"),
                    images=np.stack(shard_imgs, axis=0),  # (SHARD_SIZE, 224, 224, 3)
                    labels=np.array(shard_labs, dtype=np.int64)
                )
                shard_idx += 1
                shard_imgs, shard_labs = [], []
    # flush remainder
    if shard_imgs:
        np.savez_compressed(
            os.path.join(OUT_DIR, f"shard_{shard_idx:05d}.npz"),
            images=np.stack(shard_imgs, axis=0),
            labels=np.array(shard_labs, dtype=np.int64)
        )
    print(f"Saved {written} samples into {shard_idx + 1} shard(s) under '{OUT_DIR}'.")

# 4) Helper to lazily iterate saved shards later (no JPEG decode, no preprocessing)
def iterate_npz_shards(out_dir=OUT_DIR):
    shard_files = sorted(glob.glob(os.path.join(out_dir, "shard_*.npz")))
    for sf in shard_files:
        payload = np.load(sf)
        yield payload["images"], payload["labels"]

# Example usage (uncomment when you want to load preprocessed data lazily):
# for imgs, labs in iterate_npz_shards(OUT_DIR):
#     print("Loaded shard:", imgs.shape, labs.shape)  # e.g., (64, 224, 224, 3) (64,)
#     # You can now call model.predict(imgs) directly (Keras expects NHWC float32)











OUT_DIR = "preprocessed_npz_tf"  # where your shards live
BATCH_SIZE = 64                  # preprocessed already; you can go big
PRED_CSV = "top5_imagenet_preds.csv"

# Load class index mapping used when you saved shards
with open(os.path.join(OUT_DIR, "labels.json"), "r", encoding="utf-8") as f:
    meta = json.load(f)
classes = meta["classes"]                # list like ['african-wildcat', ...]
idx_to_class = {i:c for c,i in meta["class_to_idx"].items()}  # invert

# Load model
model = ResNet50(weights="imagenet")

# Helper to stream shards
def iterate_npz_shards(out_dir=OUT_DIR):
    shard_files = sorted(glob.glob(os.path.join(out_dir, "shard_*.npz")))
    for sf in shard_files:
        payload = np.load(sf)
        yield payload["images"], payload["labels"]   # (N,224,224,3), (N,)

# Predict and write a CSV with top-5 ImageNet labels per image
# Columns: true_class, pred1_name, pred1_prob, ..., pred5_name, pred5_prob
with open(PRED_CSV, "w", newline="", encoding="utf-8") as f:
    w = csv.writer(f)
    w.writerow(["true_class",
                "pred1_name","pred1_prob",
                "pred2_name","pred2_prob",
                "pred3_name","pred3_prob",
                "pred4_name","pred4_prob",
                "pred5_name","pred5_prob"])

    total = 0
    for imgs, labs in iterate_npz_shards(OUT_DIR):
        # imgs are already preprocessed for ResNet50
        for i in range(0, len(imgs), BATCH_SIZE):
            batch = imgs[i:i+BATCH_SIZE]
            preds = model.predict(batch, verbose=0)            # (B,1000)
            decoded = decode_predictions(preds, top=5)         # list of lists
            for d, lbl in zip(decoded, labs[i:i+BATCH_SIZE]):
                row = [idx_to_class[int(lbl)]]
                for (_, name, prob) in d:
                    row += [name, f"{prob:.6f}"]
                w.writerow(row)
                total += 1

print(f"Wrote top-5 predictions for {total} images to {PRED_CSV}")
print("Open it, scan which ImageNet labels show up for each of your classes,")
print("and decide a mapping (next step).")


get_ipython().run_cell_magic("writefile", " imagenet_to_target.csv", """imagenet_label,target_class
# Domestic cat
tabby,domestic-cat
tiger_cat,domestic-cat
Egyptian_cat,domestic-cat
Persian_cat,domestic-cat
Siamese_cat,domestic-cat
Angora,domestic-cat
# European wildcat
lynx,european-wildcat
wildcat,european-wildcat
wild_boar,european-wildcat
great_grey_owl,european-wildcat
# Jungle cat
leopard,jungle-cat
jaguar,jungle-cat
puma,jungle-cat
cougar,jungle-cat
tiger,jungle-cat
dhole,jungle-cat
dingo,jungle-cat
red_wolf,jungle-cat
# African wildcat
cheetah,african-wildcat
lion,african-wildcat
# Blackfoot cat
caracal,blackfoot-cat
ocelot,blackfoot-cat
# Chinese mountain cat
snow_leopard,chinese-mountain-cat
# Sand cat
sand_cat,sand-cat
kit_fox,sand-cat
Arctic_fox,sand-cat
white_wolf,sand-cat
red_fox,sand-cat""")


PRED_CSV = "top5_imagenet_preds.csv"
MAP_CSV  = "imagenet_to_target.csv"
UNKNOWN_CLASS = "unknown"  # bucket for unmapped predictions

# Load mapping (imagenet_label -> your target class)
imap = {}
with open(MAP_CSV, "r", encoding="utf-8") as f:
    rdr = csv.DictReader(f)
    for row in rdr:
        k = (row.get("imagenet_label") or "").strip()
        v = (row.get("target_class") or "").strip()
        if k and v:   # only add if both key and value are non-empty
            imap[k] = v

print("Loaded", len(imap), "mappings")
print(list(imap.items())[:10])  # sanity check

# Helper: given a top-5 list [(name,prob),...], map to your class
# Strategy: pick highest-prob label that exists in mapping; else UNKNOWN
def map_top5_to_target(top5_pairs):
    for name, prob in top5_pairs:   # already ordered by prob desc
        if name in imap:
            return imap[name]
    return UNKNOWN_CLASS

y_true, y_pred = [], []

with open(PRED_CSV, "r", encoding="utf-8") as f:
    rdr = csv.reader(f)
    header = next(rdr)
    # column indices
    true_idx = header.index("true_class")
    # Build structured rows
    for row in rdr:
        true_class = row[true_idx]
        # pull top-5 (name,prob) pairs
        pairs = []
        for k in [("pred1_name","pred1_prob"),
                  ("pred2_name","pred2_prob"),
                  ("pred3_name","pred3_prob"),
                  ("pred4_name","pred4_prob"),
                  ("pred5_name","pred5_prob")]:
            n_idx = header.index(k[0]); p_idx = header.index(k[1])
            name = row[n_idx]
            prob = float(row[p_idx])
            pairs.append((name, prob))
        pred_class = map_top5_to_target(pairs)
        y_true.append(true_class)
        y_pred.append(pred_class)

# Restrict labels to your known classes (plus unknown if present)
all_labels = sorted(list(set(y_true) | set(y_pred)))
if UNKNOWN_CLASS in set(y_pred):
    if UNKNOWN_CLASS not in all_labels:
        all_labels.append(UNKNOWN_CLASS)

# Metrics
print("=== Classification Report ===")
print(classification_report(y_true, y_pred, labels=all_labels, zero_division=0))

print("\n=== Confusion Matrix (rows=true, cols=pred) ===")
cm = confusion_matrix(y_true, y_pred, labels=all_labels)
# Pretty print
col_w = max(len(x) for x in all_labels) + 2
print(" " * col_w + " ".join(f"{c:>{col_w}}" for c in all_labels))
for i, r in enumerate(cm):
    print(f"{all_labels[i]:>{col_w}}" + " ".join(f"{n:>{col_w}d}" for n in r))


import csv
from collections import Counter, defaultdict
import pandas as pd

# === Config ===
PRED_CSV = "top5_imagenet_preds.csv"     # from earlier prediction step
MAP_CSV  = "imagenet_to_target.csv"      # current mapping file
TOP_K    = 30

# === 1) Load mapping (robust) ===
imap = {}
try:
    with open(MAP_CSV, "r", newline="", encoding="utf-8") as f:
        rdr = csv.DictReader(f)
        print("Mapping CSV headers:", rdr.fieldnames)
        bad_rows = 0
        for i, row in enumerate(rdr, start=2):
            k = (row.get("imagenet_label") or "").strip()
            v = (row.get("target_class")   or "").strip()
            if k and v:
                imap[k] = v
            else:
                bad_rows += 1
        if bad_rows:
            print(f"⚠️ Skipped {bad_rows} mapping row(s) with empty/missing values.")
except FileNotFoundError:
    print(f"⚠️ Mapping file '{MAP_CSV}' not found. Proceeding with empty mapping.")

# Define 'mapped' immediately after building 'imap'
mapped = set(imap.keys())

# === 2) Read predictions & tally ===
freq_all = Counter()
freq_unmapped = Counter()
per_true_unmapped = defaultdict(lambda: Counter())  # label -> (true_class -> count)

with open(PRED_CSV, "r", encoding="utf-8", newline="") as f:
    rdr = csv.DictReader(f)
    if not rdr.fieldnames:
        raise ValueError(f"No headers found in {PRED_CSV}")
    top_cols = [c for c in rdr.fieldnames if c and c.startswith("pred") and c.endswith("_name")]
    if "true_class" not in rdr.fieldnames:
        raise KeyError("Column 'true_class' missing from predictions CSV")

    for row in rdr:
        true_cls = (row.get("true_class") or "").strip()
        labels = [(row.get(c) or "").strip() for c in top_cols]
        for lbl in labels:
            if not lbl:
                continue
            freq_all[lbl] += 1
            if lbl not in mapped:
                freq_unmapped[lbl] += 1
                if true_cls:
                    per_true_unmapped[lbl][true_cls] += 1

# === 3) Print top-N unmapped labels ===
print(f"\n=== Top {TOP_K} UNMAPPED labels (overall frequency across top-5) ===")
for lbl, cnt in freq_unmapped.most_common(TOP_K):
    print(f"{lbl:20s}  {cnt}")

# === 4) Per-true-class breakdown for those labels ===
def top_k_true_classes(counter, k=5):
    return ", ".join(f"{cls}:{n}" for cls, n in counter.most_common(k))

print(f"\n=== Per-true-class breakdown for top {TOP_K} unmapped labels ===")
for lbl, cnt in freq_unmapped.most_common(TOP_K):
    breakdown = top_k_true_classes(per_true_unmapped[lbl], k=5)
    print(f"{lbl:20s}  total={cnt:4d}  ->  {breakdown}")

# === 5) Write helper CSV with mapping candidates ===
candidates = []
for lbl, cnt in freq_unmapped.most_common():
    breakdown = per_true_unmapped[lbl].most_common(5)
    top_true = "; ".join([f"{c}:{n}" for c, n in breakdown])
    candidates.append({
        "imagenet_label": lbl,
        "suggested_target": "",  # fill manually
        "frequency": cnt,
        "top_true_classes": top_true
    })

cand_df = pd.DataFrame(candidates)
cand_df.to_csv("mapping_candidates.csv", index=False)
print("\nSaved mapping_candidates.csv with columns: imagenet_label, suggested_target, frequency, top_true_classes")

# === 6) Quick sanity: how many of top-50 labels are already mapped? ===
mapped_in_top50 = sum(1 for lbl, _ in freq_all.most_common(50) if lbl in mapped)
print(f"\nMapped within top-50 most common labels: {mapped_in_top50}/50")
























